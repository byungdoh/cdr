# ANNOTATED CDRNN MODEL FILE TEMPLATE.

# The CDRNN model file gives lots of control over the design.
# The default settings provided by this template are reasonable
# for the analyses we have tried, but as always, domain knowledge
# may motivate deviation in one or more settings.
# Please carefully read this template and consult the documentation
# (https://cdr.readthedocs.io/en/latest/) before running.

# The main inputs provided by the user are (1) paths to data, (2) an output
# directory, and (3) one or more model specifications. Values provided as "<>"
# are placeholders that must be filled in by the user.
# Fields marked as OPTIONAL below may be safely commented out.
# Fields marked as DEFAULT below may be commented out, in which case
# values will revert to repository defaults, which may differ from
# the defaults provided here.

# This repository is under active development. As a result, the details
# of this template are subject to change in future releases.



# DATA
[data]
# IMPORTANT: If train/dev/test partition the same dataset, X_train, X_dev, X_test should be THE SAME.
#            Only partition the responses.

# REQUIRED
# Path(s) to train set predictor tables (;-delimited if more than 1)
X_train = <PATH>(;<PATH)*
# Path to training set response table
y_train = <PATH>
# Space-delimited columns distinguishing unique time series
# (must all be present in all predictor and response tables).
# E.g. series_ids = participant experiment
series_ids = <COL>( <COL)*

# OPTIONAL
# Path(s) to dev set predictor tables (;-delimited if more than 1)
X_dev = <PATH>(;<PATH)*
# Path to dev set response table
y_dev = <PATH>
# Path(s) to test set predictor tables (;-delimited if more than 1)
X_test = <PATH>(;<PATH)*
# Path to test set response table
y_test = <PATH>
# Data retention conditions (;-delimited).
# Response data that fail any of these conditions will be dropped from fitting/evaluation.
# E.g. filters = y >= 1; y <= 100;
filters = <COND>(;<COND)*

# DEFAULT
# Column delimiter (defaults to ' ')
sep = ' '
# Number of preceding timesteps used in convolution step
history_length = 128




# GLOBAL SETTINGS
[global_settings]
# REQUIRED
# Path to output directory
outdir = ../results/natfmri_new2/lang_dropfinal




# CDR SETTINGS
[cdr_settings]
# DEFAULT (see docs for available options)
network_type = mle
optim_name = Nadam
n_iter = 15000
epsilon = 0.01
optim_epsilon = 1.0
learning_rate = 0.001
max_global_gradient_norm = 1
loss_filter_n_sds = 10000
minibatch_size = 128
eval_minibatch_size = 10000
convergence_n_iterates = 25
nn_regularizer_name = l2_regularizer
nn_regularizer_scale = 1e-3
context_regularizer_name = l1_l2_regularizer
context_regularizer_scale = 1e-2
ranef_regularizer_name = l2_regularizer
ranef_regularizer_scale = 1e-3
scale_regularizer_with_data = True
log_freq = 1
save_freq = 10

generate_irf_surface_plots = True
generate_curvature_plots = True




# IRF NAME MAP
# The entire `irf_name_map` section is optional and used only for plotting
# It provides a map from hard-to-read estimate names in the model specification
# to user-selected names for plotting, making plots more readable.
# Example:
# [irf_name_map]
# DiracDelta.s(var1)-Terminal.s(var1) = Variable 1
# ShiftedGammaShapeGT1.s(var1)-Terminal.s(var1) = Variable 2




# MODEL(S)
# Arbitrarily many models can be specified below.
# They will all share the same inputs ([data] section above).
# To use different inputs, a different config file must be written.
# Each model has a corresponding section named as follows:
#
# [model_CDR(<SUFFIX>)]
#
# where the optional <SUFFIX> is a user-provided nickname for the model.
# All model sections contain at minimum a `formula` field specifying the model.
# Models may additionally provide an `ablate` field containing a space-delimited list
# of predictors to ablate. This will generate a unique model for the powerset of
# ablate, representing each logically possible ablation.
# Values from [cdr_settings] above will be inherited unless they are locally overridden.
# For example, to change the learning rate for a specific model, simply
# specify a `learning_rate = ...` field in the model's section.
#
# For details about CDR model formula syntax, see docs.
#
# Examples:
#
# [model_CDR_var1]
# formula = y ~ C(var1, Normal())
#
# [model_CDR_var12_mle]
# network_type = mle
# ablate = var1 var2
# formula = y ~ C(var1 + var2, Normal())
























[data]
X_train = ~/dtsr/experiments_emnlp18/natstor_X.csv
X_dev = ~/dtsr/experiments_emnlp18/natstor_X.csv
X_test = ~/dtsr/experiments_emnlp18/natstor_X.csv
y_train = ~/dtsr/experiments_emnlp18/natstor_y_train.csv
y_dev = ~/dtsr/experiments_emnlp18/natstor_y_dev.csv
y_test = ~/dtsr/experiments_emnlp18/natstor_y_test.csv
series_ids = subject docid
split_ids = subject sentid
modulus = 4
history_length = 128
filters = fdur > 100; fdur < 3000; correct > 4; startofsentence != 1; endofsentence != 1; subjectnunique > 100

[global_settings]
outdir = ../results/cdrnn/natspr
use_gpu_if_available = False

[cdr_settings]
network_type = bayes
declare_priors = False
ranef_regularizer_name = l2_regularizer
ranef_regularizer_scale = 1.
max_global_gradient_norm = 1.

n_iter = 15000
n_sample = 1
learning_rate = 0.001
log_freq = 10
save_freq = 10

generate_irf_surface_plots = True
generate_curvature_plots = True

[irf_name_map]
rate = Rate
s(sentpos) = Sentence position
s(wlen) = Word length
s(unigram) = Unigram logprob
s(fwprob5surp) = 5-gram surprisal

[model_CDRNN]
ablate = unigramsurp fwprob5surp
network_type = mle
optim_name = Nadam
epsilon = 0.01
optim_epsilon = 1.0
learning_rate = 0.001
max_global_gradient_norm = 1
loss_filter_n_sds = 10000
minibatch_size = 128
eval_minibatch_size = 10000
convergence_n_iterates = 25
nn_regularizer_name = l2_regularizer
nn_regularizer_scale = 1e-3
context_regularizer_name = l1_l2_regularizer
context_regularizer_scale = 1e-2
ranef_regularizer_name = l2_regularizer
ranef_regularizer_scale = 1e-3
scale_regularizer_with_data = True
log_freq = 1
save_freq = 10
formula = log(fdur) ~ s(wlen) + s(unigramsurp) + s(fwprob5surp) + (1 | subject)

[model_CDRNN_asym]
network_type = mle
asymmetric_error = True
optim_name = Nadam
epsilon = 0.01
optim_epsilon = 1.0
learning_rate = 0.001
max_global_gradient_norm = 1
loss_filter_n_sds = 10000
minibatch_size = 128
eval_minibatch_size = 10000
convergence_n_iterates = 25
nn_regularizer_name = l2_regularizer
nn_regularizer_scale = 1e-3
context_regularizer_name = l1_l2_regularizer
context_regularizer_scale = 1e-2
ranef_regularizer_name = l2_regularizer
ranef_regularizer_scale = 1e-3
scale_regularizer_with_data = True
log_freq = 1
save_freq = 10
formula = log(fdur) ~ s(wlen) + s(unigramsurp) + s(fwprob5surp) + (1 | subject)

[model_CDRNN_noreg]
network_type = mle
optim_name = Nadam
epsilon = 0.01
optim_epsilon = 1.0
learning_rate = 0.001
max_global_gradient_norm = 1
loss_filter_n_sds = 10000
minibatch_size = 128
eval_minibatch_size = 10000
convergence_n_iterates = 25
log_freq = 1
save_freq = 10
formula = log(fdur) ~ s(wlen) + s(unigramsurp) + s(fwprob5surp) + (1 | subject)

[model_CDRNN_asym_noreg]
network_type = mle
asymmetric_error = True
optim_name = Nadam
epsilon = 0.01
optim_epsilon = 1.0
learning_rate = 0.001
max_global_gradient_norm = 1
loss_filter_n_sds = 10000
minibatch_size = 128
eval_minibatch_size = 10000
convergence_n_iterates = 25
log_freq = 1
save_freq = 10
formula = log(fdur) ~ s(wlen) + s(unigramsurp) + s(fwprob5surp) + (1 | subject)

[model_CDRNN_dlt]
ablate = dlt
network_type = mle
optim_name = Nadam
epsilon = 0.01
optim_epsilon = 1.0
learning_rate = 0.001
max_global_gradient_norm = 1
loss_filter_n_sds = 10000
minibatch_size = 128
eval_minibatch_size = 10000
convergence_n_iterates = 25
nn_regularizer_name = l2_regularizer
nn_regularizer_scale = 1e-3
context_regularizer_name = l1_l2_regularizer
context_regularizer_scale = 1e-2
ranef_regularizer_name = l2_regularizer
ranef_regularizer_scale = 1e-3
scale_regularizer_with_data = True
log_freq = 1
save_freq = 10
formula = log(fdur) ~ s(wlen) + s(unigramsurp) + s(fwprob5surp) + s(totsurp) + s(adaptivesurp) + s(yesJ) + s(dltcvm) + (1 | subject)